torch>=2.3.1
transformers
python-box
einops
omegaconf
pytorch_lightning
lightning
addict
timm
fast-simplification
bpy>=3.6.0
trimesh
open3d
pyrender
huggingface_hub
numpy==1.26.4
scipy
matplotlib
plotly
pyyaml
spaces
https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.2cxx11abiFALSE-cp310-cp310-linux_x86_64.whl